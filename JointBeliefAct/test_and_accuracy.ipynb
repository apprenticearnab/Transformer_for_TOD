{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from tokenizers import ByteLevelBPETokenizer\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math, copy, time\n",
    "from torch.autograd import Variable\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataset import encoding\n",
    "from model_joint import make_model\n",
    "from model_joint import LabelSmoothing\n",
    "from model_joint import NoamOpt\n",
    "from model_joint import data_gen\n",
    "from belief_accuracy import joint_accuracy\n",
    "from belief_accuracy import slot_accuracy\n",
    "from action_belief_accuracy import obtain_TP_TN_FN_FP\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = ByteLevelBPETokenizer('simpletod/vocab.json','simpletod/merges.txt')\n",
    "tokenizer.add_special_tokens([\"<PAD>\",\"<|belief|>\",\" <|endofbelief|>\",\"<|context|>\",\" <|user|> \",\" <|system|> \",\"<|endofcontext|>\",\"<|action|>\",\" <|endofaction|>\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18157\n"
     ]
    }
   ],
   "source": [
    "pad_tok = tokenizer.encode(\"<PAD>\").ids.pop()\n",
    "print(pad_tok)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_ele(elem_list):\n",
    "  for ele in elem_list:\n",
    "    if tokenizer.decode([ele],skip_special_tokens=False) == '<PAD>':\n",
    "      return\n",
    "    yield tokenizer.decode([ele],skip_special_tokens=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_epoch(data_iter, model, loss_compute):\n",
    "    \"Standard Training and Logging Function\"\n",
    "    start = time.time()\n",
    "    total_tokens = 0\n",
    "    total_loss = 0\n",
    "    tokens = 0\n",
    "    loss1 = []\n",
    "    index = []\n",
    "    ref1 = []\n",
    "    cand1 = []\n",
    "    #file1 = open(\"test_file2.txt\", \"a\")\n",
    "    for i, batch in enumerate(data_iter):\n",
    "        reference = []\n",
    "        candidate = []\n",
    "        ref2 = []\n",
    "        cand2 = []\n",
    "        out_belief, out_action = model.forward_belief_action(batch.src.to(device), batch.trg1.to(device), \n",
    "                            batch.src_mask.to(device), batch.trg1_mask.to(device),batch.trg2.to(device),batch.trg2_mask.to(device))\n",
    "        #out_action = model.forward_action(batch.src.to(device), batch.trg2.to(device), \n",
    "                            #batch.src_mask.to(device), batch.trg2_mask.to(device))\n",
    "        loss = loss_compute(out_belief,out_action, batch.trg1_y.to(device),batch.trg2_y.to(device), batch.ntokens1.to(device),batch.ntokens2.to(device))\n",
    "        loss1.append(loss)\n",
    "        index.append(i)\n",
    "        total_loss += loss\n",
    "        total_tokens += (batch.ntokens1+batch.ntokens2)\n",
    "        tokens += (batch.ntokens1+batch.ntokens2)\n",
    "        #file1.write(\"Batch No. - %d \\n\" % (i+1))\n",
    "        #max , idx = torch.max(model.generator(out),dim=2)\n",
    "        #pred2 = idx.tolist()\n",
    "        #for d in range(len(batch.src.tolist())):\n",
    "                #file1.write(''.join(list(print_ele(batch.src.tolist()[d]))) + \"\\n\")#tokenizer.decode(batch.src.tolist()[d],skip_special_tokens=False) + \"\\n\")\n",
    "                #file1.write(\"<|belief|>\" + tokenizer.decode(pred2[d],skip_special_tokens=True) + \" <|endofbelief|>\" + \"\\n\")\n",
    "                #file1.write(\"<|belief|>\" + tokenizer.decode(batch.trg_y.tolist()[d],skip_special_tokens=True) + \" <|endofbelief|>\" + \"\\n\")\n",
    "                #ref1.append(tokenizer.decode(pred2[d],skip_special_tokens=True).split(','))\n",
    "                #cand1.append(tokenizer.decode(batch.trg_y.tolist()[d],skip_special_tokens=True).split(','))\n",
    "                #ref2.append(tokenizer.decode(pred2[d],skip_special_tokens=True).split(','))\n",
    "                #cand2.append(tokenizer.decode(batch.trg_y.tolist()[d],skip_special_tokens=True).split(','))\n",
    "        #file1.write(\"Joint Accuracy - %f \\n\" % joint_accuracy(cand2,ref2,d_c = False,type2_c = False))\n",
    "        #file1.write(\"Slot Accuracy - %f \\n\" % slot_accuracy(cand2,ref2))\n",
    "        if i % 200 == 1:\n",
    "            elapsed = time.time() - start\n",
    "            '''max , idx = torch.max(model.generator(out),dim=2)\n",
    "            a = idx.tolist()\n",
    "            b = batch.trg_y.tolist()\n",
    "            print(idx.size())\n",
    "            print(batch.trg_y.size())\n",
    "            for id in a:\n",
    "                reference.append(tokenizer.decode(id,skip_special_tokens=True).split(','))\n",
    "            for p in batch.trg_y.tolist():\n",
    "                candidate.append(tokenizer.decode(p,skip_special_tokens=True).split(',')) \n",
    "            print(reference)\n",
    "            print(candidate)\n",
    "            accuracy = joint_accuracy(candidate,reference,d_c = False,type2_c = False)\n",
    "            slt_accuracy = slot_accuracy(candidate,reference)'''\n",
    "            print(\"Epoch Step: %d Loss: %f Tokens per Sec: %f \" %\n",
    "                    (i, loss / (batch.ntokens1+batch.ntokens2), tokens / elapsed))\n",
    "            start = time.time()\n",
    "            tokens = 0\n",
    "    #file1.write(\"Overall Joint Accuracy - %f \\n\" % joint_accuracy(cand1,ref1,d_c = False,type2_c = False))\n",
    "    #file1.write(\"Overall Slot Accuracy - %f \\n\" % slot_accuracy(cand1,ref1))\n",
    "    #file1.close()\n",
    "    return total_loss / total_tokens#,accuracy,slt_accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Loss Compute Cell**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleLossCompute:\n",
    "    \"A simple loss compute and train function.\"\n",
    "    def __init__(self, generator,generator1, criterion, opt=None):\n",
    "        self.generator = generator\n",
    "        self.generator1 = generator1\n",
    "        self.criterion = criterion\n",
    "        self.opt = opt\n",
    "        \n",
    "    def __call__(self, x1,x2, y,z, norm1,norm2):\n",
    "        x1 = self.generator(x1)\n",
    "        x2 = self.generator1(x2)\n",
    "        loss1 = self.criterion(x1.contiguous().view(-1, x1.size(-1)), \n",
    "                              y.contiguous().view(-1)) / norm1\n",
    "        loss2 = self.criterion(x2.contiguous().view(-1, x2.size(-1)), \n",
    "                              z.contiguous().view(-1)) / norm2\n",
    "        loss = loss1 + loss2\n",
    "        loss.backward()\n",
    "        if self.opt is not None:\n",
    "            self.opt.step()\n",
    "            self.opt.optimizer.zero_grad()\n",
    "        return loss.item() * norm1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "V = 30000\n",
    "model = torch.load('simpletod/dialog_NLP.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Initial one**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\alipu\\anaconda3\\lib\\site-packages\\torch\\nn\\_reduction.py:44: UserWarning: size_average and reduce args will be deprecated, please use reduction='sum' instead.\n",
      "  warnings.warn(warning.format(ret))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch Step: 1 Loss: 11.673458 Tokens per Sec: 1564.010620 \n",
      "Epoch Step: 201 Loss: 11.669713 Tokens per Sec: 2995.048828 \n",
      "Epoch Step: 401 Loss: 11.673076 Tokens per Sec: 8254.501953 \n",
      "Epoch Step: 601 Loss: 11.660394 Tokens per Sec: 10573.483398 \n",
      "tensor(11.6703, device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "criterion = LabelSmoothing(size=V, padding_idx=0, smoothing=0.0)\n",
    "\n",
    "\n",
    "s7,t7,a7 = encoding('simpletod/Processed_data/simptod/context_test.txt','simpletod/Processed_data/simptod/belief_test.txt','simpletod/Processed_data/simptod/action_test.txt',400)\n",
    "l2 = run_epoch(data_gen(V, 10, 737,s7,t7,a7), model.to(device), \n",
    "                    SimpleLossCompute(model.generator,model.generator1, criterion, None))\n",
    "print(l2)\n",
    "#print(a3)\n",
    "#print(sa3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Final one**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For Belief prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = tokenizer.encode(\"<|belief|>\").ids.pop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def greedy_decode(model, src, src_mask, max_len, start_symbol):\n",
    "    memory = model.encode(src, src_mask)\n",
    "    words = torch.ones(1, 1).fill_(start_symbol).type_as(src.data)\n",
    "    for i in range(max_len-1):\n",
    "        out_belief = model.decode_belief(memory, src_mask, \n",
    "                           Variable(words), \n",
    "                           Variable(subsequent_mask(words.size(1))\n",
    "                                    .type_as(src.data)))\n",
    "        prob = model.generator(out_belief[:, -1])\n",
    "        _, next_word = torch.max(prob, dim = 1)\n",
    "        next_word = next_word.item()\n",
    "        words = torch.cat([words, \n",
    "                        torch.ones(1, 1).type_as(src.data).fill_(next_word)], dim=1)\n",
    "    if words.dim() == 2:\n",
    "        words = words.squeeze(0)\n",
    "        words = words.tolist()\n",
    "        \n",
    "    sen_idx = [w for w in words]\n",
    "    sentence = tokenizer.decode(sen_idx,skip_special_tokens=True)#' '.join([rev_word_map[sen_idx[k]] for k in range(len(sen_idx))])\n",
    "    \n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s4,t4,a4= encoding('simpletod/Processed_data/simptod/context_test.txt','simpletod/Processed_data/simptod/belief_test.txt','simpletod/Processed_data/simptod/action_test.txt',400)\n",
    "file1 = open(\"test_file_belief.txt\", \"a\")\n",
    "for i in range(7370):\n",
    "    src = Variable(torch.unsqueeze(s4[i,:400],0))\n",
    "    src_mask = Variable(torch.ones(1, 1, 400) )\n",
    "    s = greedy_decode(model.to(device), src.to(device), src_mask.to(device), 60, start)\n",
    "    file1.write(''.join(list(print_ele([w for w in s4[i][:400]]))) + \"\\n\")#tokenizer.decode(batch.src.tolist()[d],skip_special_tokens=False) + \"\\n\")\n",
    "    file1.write(\"<|belief|>\" + s + \" <|endofbelief|>\" + \"\\n\")\n",
    "    file1.write(\"<|belief|>\" + tokenizer.decode([w for w in t4[i][:400]],skip_special_tokens=True) + \" <|endofbelief|>\" + \"\\n\")\n",
    "file1.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For Action prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = tokenizer.encode(\"<|action|>\").ids.pop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def greedy_decode(model, src, src_mask, max_len, start_symbol):\n",
    "    memory = model.encode(src, src_mask)\n",
    "    words = torch.ones(1, 1).fill_(start_symbol).type_as(src.data)\n",
    "    for i in range(max_len-1):\n",
    "        out_action = model.decode_action(memory, src_mask, \n",
    "                           Variable(words), \n",
    "                           Variable(subsequent_mask(words.size(1))\n",
    "                                    .type_as(src.data)))\n",
    "        prob = model.generator1(out_action[:, -1])\n",
    "        _, next_word = torch.max(prob, dim = 1)\n",
    "        next_word = next_word.item()\n",
    "        words = torch.cat([words, \n",
    "                        torch.ones(1, 1).type_as(src.data).fill_(next_word)], dim=1)\n",
    "    if words.dim() == 2:\n",
    "        words = words.squeeze(0)\n",
    "        words = words.tolist()\n",
    "        \n",
    "    sen_idx = [w for w in words]\n",
    "    sentence = tokenizer.decode(sen_idx,skip_special_tokens=True)#' '.join([rev_word_map[sen_idx[k]] for k in range(len(sen_idx))])\n",
    "    \n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file1 = open(\"test_file_action.txt\", \"a\")\n",
    "for i in range(7370):\n",
    "    src = Variable(torch.unsqueeze(s4[i,:400],0))\n",
    "    src_mask = Variable(torch.ones(1, 1, 400) )\n",
    "    s = greedy_decode(model.to(device), src.to(device), src_mask.to(device), 60, start)\n",
    "    file1.write(''.join(list(print_ele([w for w in s4[i][:400]]))) + \"\\n\")#tokenizer.decode(batch.src.tolist()[d],skip_special_tokens=False) + \"\\n\")\n",
    "    file1.write(\"<|action|>\" + s + \" <|endofaction|>\" + \"\\n\")\n",
    "    file1.write(\"<|action|>\" + tokenizer.decode([w for w in a4[i][:400]],skip_special_tokens=True) + \" <|endofaction|>\" + \"\\n\")\n",
    "file1.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Joint Accuracy and Slot Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "f=open('test_file_belief.txt','r',errors = 'ignore')\n",
    "raw=f.read()\n",
    "#raw=raw.lower()\n",
    "output = raw.split(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict = {'target_turn_belief':[] ,'generated_turn_belief':[] ,'model_context':[] }\n",
    "#dict = dict.fromkeys(['target_turn_belief','generated_turn_belief','model_context'])\n",
    "for i in range(0,len(output)-3,3):\n",
    "    dict['target_turn_belief'].append((' '.join(output[i+2].split()[1:-1]).split(',')))\n",
    "    dict['generated_turn_belief'].append((' '.join(output[i+1].split()[1:-1]).split(',')))\n",
    "    dict['model_context'].append((' '.join(output[i].split()[1:-1]).split(',')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict1 = {i:{'target_turn_belief':dict['target_turn_belief'][i:i+10],'generated_turn_belief':dict['generated_turn_belief'][i:i+10],'model_context':dict['model_context'][i:i+10]} for i in range(0,len(dict['target_turn_belief']),10)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open(\"eval_file_belief.json\", \"w\") as outfile:  \n",
    "    json.dump(dict1, outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "joint accuracy: 0.15082382762991128\n"
     ]
    }
   ],
   "source": [
    "!python simpletod/compute_joint_acc.py --eval_file eval_file_belief.json --type2_cleaning "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "joint accuracy: 0.13688212927756654\n"
     ]
    }
   ],
   "source": [
    "!python simpletod/compute_joint_acc.py --eval_file eval_file_belief.json "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "tgt_belief = []\n",
    "pred_belief = []\n",
    "for i in range(0,len(output)-3,3):\n",
    "    tgt_belief.append((' '.join(output[i+2].split()[1:-1]).split(',')))\n",
    "    pred_belief.append((' '.join(output[i+1].split()[1:-1]).split(',')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "slot_accuracy = slot_accuracy(tgt_belief,pred_belief)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.42775665399239543\n"
     ]
    }
   ],
   "source": [
    "print(slot_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Action accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "f=open('test_file_action.txt','r',errors = 'ignore')\n",
    "raw=f.read()\n",
    "#raw=raw.lower()\n",
    "output = raw.split(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "tgt_action = []\n",
    "pred_action = []\n",
    "for i in range(0,len(output)-3,3):\n",
    "    tgt_action.append(''.join((' '.join(output[i+2].split()[1:-1]).replace(',',''))).split())\n",
    "    pred_action.append(''.join((' '.join(output[i+1].split()[1:-1]).replace(',',''))).split())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modifications are done in this cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred =[]\n",
    "tgt = []\n",
    "for i in range(len(pred_action)):\n",
    "    #if len(tgt_action[i])==len(pred_action[i]):\n",
    "        for j in range(0,len(tgt_action[i]),3):\n",
    "            pred.append(' '.join(pred_action[i][j:j+3]))\n",
    "            tgt.append(' '.join(tgt_action[i][j:j+3]))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "domains = ['restaurant', 'hotel', 'attraction', 'train', 'taxi', 'hospital', 'police', 'bus', 'booking', 'general']\n",
    "functions = ['inform', 'request', 'recommend', 'book', 'select', 'sorry', 'none','reqmore']\n",
    "arguments = ['pricerange', 'id', 'addr', 'post', 'type','', 'food', 'phone', 'name', 'area', 'choice', \n",
    "             'price', 'time', 'ref',  'parking', 'stars', 'internet', 'day', 'arrive', 'depart',\n",
    "             'dest', 'leave','greet', 'duration', 'trainid','ticket', 'people', 'department', 'stay','car','bye','fee','nobook','nooffer','offerbook','welcome','offerbooked']\n",
    "used_levels = domains + functions + arguments\n",
    "keys = range(len(used_levels))\n",
    "act_len = len(used_levels)\n",
    "#pre1 = [0] * act_len\n",
    "#pre2 = [0] * act_len\n",
    "\n",
    "dicts = {}\n",
    "for p in keys:\n",
    "    dicts[p] = used_levels[p]\n",
    "dict = {v: k for k, v in dicts.items()}\n",
    "\n",
    "all_pred = []\n",
    "all_label = []\n",
    "\n",
    "for i in pred:\n",
    "    pre1 = [0] * act_len\n",
    "    for w in i.split(' '):\n",
    "        pre1[dict[w]] = 1\n",
    "    all_pred.append(pre1)\n",
    "for j in tgt:\n",
    "    pre2 = [0] * act_len\n",
    "    for w in j.split(' '):\n",
    "        pre2[dict[w]] = 1\n",
    "    all_label.append(pre2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_pred = torch.Tensor(all_pred)\n",
    "all_label = torch.Tensor(all_label)\n",
    "\n",
    "TP, TN, FN, FP = 0 ,0,0,0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "TP, TN, FN, FP = obtain_TP_TN_FN_FP(all_pred, all_label, TP, TN, FN, FP)\n",
    "precision = TP / (TP + FP + 0.001)\n",
    "recall = TP / (TP + FN + 0.001)\n",
    "F1 = 2 * precision * recall / (precision + recall + 0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24776\n"
     ]
    }
   ],
   "source": [
    "print(TP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.514227589418493\n"
     ]
    }
   ],
   "source": [
    "print(precision)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.43211942876867193\n"
     ]
    }
   ],
   "source": [
    "print(recall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.46911581063890434\n"
     ]
    }
   ],
   "source": [
    "print(F1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
