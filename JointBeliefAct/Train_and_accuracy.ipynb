{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "EuScVXeVZgrG"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in c:\\users\\alipu\\anaconda3\\lib\\site-packages (from -r requirements.txt (line 1)) (3.5.1)\n",
      "Collecting spicy\n",
      "  Using cached spicy-0.16.0-py2.py3-none-any.whl (1.7 kB)\n",
      "Collecting boto3\n",
      "  Downloading boto3-1.16.26-py2.py3-none-any.whl (129 kB)\n",
      "Requirement already satisfied: ipdb in c:\\users\\alipu\\anaconda3\\lib\\site-packages (from -r requirements.txt (line 4)) (0.13.4)\n",
      "Requirement already satisfied: tqdm in c:\\users\\alipu\\anaconda3\\lib\\site-packages (from -r requirements.txt (line 5)) (4.50.2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Could not find a version that satisfies the requirement json (from -r requirements.txt (line 6)) (from versions: none)\n",
      "ERROR: No matching distribution found for json (from -r requirements.txt (line 6))\n"
     ]
    }
   ],
   "source": [
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from tokenizers import ByteLevelBPETokenizer\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math, copy, time\n",
    "from torch.autograd import Variable\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataset import encoding\n",
    "from model_joint import make_model\n",
    "from model_joint import LabelSmoothing\n",
    "from model_joint import NoamOpt\n",
    "from model_joint import data_gen\n",
    "from belief_accuracy import joint_accuracy\n",
    "from belief_accuracy import slot_accuracy\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Dataloader cell**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "s1,t1,a1= encoding('simpletod/Processed_data/simptod/context_train.txt','simpletod/Processed_data/simptod/belief_train.txt','simpletod/Processed_data/simptod/action_train.txt',400)    #Train data\n",
    "s9,t9,a9 = encoding('simpletod/Processed_data/simptod/context_val.txt','simpletod/Processed_data/simptod/belief_val.txt','simpletod/Processed_data/simptod/action_val.txt',400)        #Validation data\n",
    "s4,t4,a4 = encoding('simpletod/Processed_data/simptod/context_test.txt','simpletod/Processed_data/simptod/belief_test.txt','simpletod/Processed_data/simptod/action_test.txt',400)      #Test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vov2M0ZsttpV",
    "outputId": "aba04d53-7057-4773-d3a8-ad4a5df8e90d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2VCw6Fe471aB",
    "outputId": "306cf54d-dac3-46c6-cc3b-1f9f88fb7b78"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#can be ignored\n",
    "tokenizer = ByteLevelBPETokenizer('vocab.json','merges.txt')\n",
    "tokenizer.add_special_tokens([\"<PAD>\",\"<|belief|>\",\" <|endofbelief|>\",\"<|context|>\",\" <|user|> \",\" <|system|> \",\"<|endofcontext|>\",\"<|action|>\",\" <|endofaction|>\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "DuaTOx7rOUBX"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18157\n"
     ]
    }
   ],
   "source": [
    "pad_tok = tokenizer.encode(\"<PAD>\").ids.pop()\n",
    "print(pad_tok)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "k1KH-_gncK1s"
   },
   "outputs": [],
   "source": [
    "def run_epoch(data_iter, model, loss_compute):\n",
    "    \"Standard Training and Logging Function\"\n",
    "    start = time.time()\n",
    "    total_tokens = 0\n",
    "    total_loss = 0\n",
    "    tokens = 0\n",
    "    loss1 = []\n",
    "    index = []\n",
    "    ref1 = []\n",
    "    cand1 = []\n",
    "    #file1 = open(\"test_res6.txt\", \"a\")\n",
    "    for i, batch in enumerate(data_iter):\n",
    "        reference = []\n",
    "        candidate = []\n",
    "        reference1 = []\n",
    "        candidate1 = []\n",
    "        ref2 = []\n",
    "        cand2 = []\n",
    "        out_belief, out_action = model.forward_belief_action(batch.src.to(device), batch.trg1.to(device), \n",
    "                            batch.src_mask.to(device), batch.trg1_mask.to(device),batch.trg2.to(device),batch.trg2_mask.to(device))\n",
    "        #out_action = model.forward_action(batch.src.to(device), batch.trg2.to(device), \n",
    "                            #batch.src_mask.to(device), batch.trg2_mask.to(device))\n",
    "        loss = loss_compute(out_belief,out_action, batch.trg1_y.to(device),batch.trg2_y.to(device), batch.ntokens1.to(device),batch.ntokens2.to(device))\n",
    "        loss1.append(loss)\n",
    "        index.append(i)\n",
    "        total_loss += loss\n",
    "        total_tokens += (batch.ntokens1+batch.ntokens2)\n",
    "        tokens += (batch.ntokens1+batch.ntokens2)\n",
    "        #file1.write(\"Batch No. - %d \\n\" % (i+1))\n",
    "        #max , idx = torch.max(model.generator1(out_action),dim=2)\n",
    "        #pred2 = idx.tolist()\n",
    "        #for d in range(len(batch.src.tolist())):\n",
    "                #file1.write(''.join(list(print_ele(batch.src.tolist()[d]))) + \"\\n\")#tokenizer.decode(batch.src.tolist()[d],skip_special_tokens=False) + \"\\n\")\n",
    "                #file1.write(\"<|belief|>\" + tokenizer.decode(pred2[d],skip_special_tokens=True) + \" <|endofbelief|>\" + \"\\n\")\n",
    "                #file1.write(\"<|belief|>\" + tokenizer.decode(batch.trg_y.tolist()[d],skip_special_tokens=True) + \" <|endofbelief|>\" + \"\\n\")\n",
    "                #file1.write(\"<|action|>\" + tokenizer.decode(pred2[d],skip_special_tokens=True) + \" <|endofaction|>\" + \"\\n\")\n",
    "                #file1.write(\"<|action|>\" + tokenizer.decode(batch.trg2_y.tolist()[d],skip_special_tokens=True) + \" <|endofaction|>\" + \"\\n\")\n",
    "                #ref1.append(tokenizer.decode(pred2[d],skip_special_tokens=True).split(','))\n",
    "                #cand1.append(tokenizer.decode(batch.trg_y.tolist()[d],skip_special_tokens=True).split(','))\n",
    "                #ref2.append(tokenizer.decode(pred2[d],skip_special_tokens=True).split(','))\n",
    "                #cand2.append(tokenizer.decode(batch.trg_y.tolist()[d],skip_special_tokens=True).split(','))\n",
    "        #file1.write(\"Joint Accuracy - %f \\n\" % joint_accuracy(cand2,ref2,d_c = False,type2_c = False))\n",
    "        #file1.write(\"Slot Accuracy - %f \\n\" % slot_accuracy(cand2,ref2))\n",
    "        if i % 200 == 1:\n",
    "            elapsed = time.time() - start\n",
    "            \"\"\"\"max1 , idx1 = torch.max(model.generator(out_belief),dim=2)\n",
    "            max2 , idx2 = torch.max(model.generator1(out_action),dim=2)\n",
    "\n",
    "            a1 = idx1.tolist()\n",
    "            b1 = batch.trg1_y.tolist()\n",
    "            a2 = idx2.tolist()\n",
    "            b2 = batch.trg2_y.tolist()\n",
    "\n",
    "            #print(idx.size())\n",
    "            #print(batch.trg_y.size())\n",
    "            for j in range(len(a1)):\n",
    "                reference.append(tokenizer.decode(a1[j],skip_special_tokens=True).split(','))\n",
    "                candidate.append(tokenizer.decode(b1[j],skip_special_tokens=True).split(','))\n",
    "            for p in range(len(a2)):\n",
    "                reference1.append(tokenizer.decode(a2[p],skip_special_tokens=True).split(','))\n",
    "                candidate1.append(tokenizer.decode(b2[p],skip_special_tokens=True).split(',')) \n",
    "            print(reference)\n",
    "            print(candidate)\n",
    "            print(reference1)\n",
    "            print(candidate1)\"\"\"\"\n",
    "            accuracy_belief = joint_accuracy(candidate,reference,d_c = False,type2_c = False)\n",
    "            slt_accuracy_belief = slot_accuracy(candidate,reference)\n",
    "            #accuracy_action = joint_accuracy(candidate1,reference1,d_c = False,type2_c = False)\n",
    "            #slt_accuracy_action = slot_accuracy(candidate1,reference1)\n",
    "\n",
    "            print(\"Epoch Step: %d Loss: %f Tokens per Sec: %f Joint_belief: %f Slot_belief: %f\" %\n",
    "                    (i, loss / (batch.ntokens1+batch.ntokens2), tokens / elapsed,accuracy_belief, slt_accuracy_belief))\n",
    "            start = time.time()\n",
    "            tokens = 0\n",
    "    #file1.write(\"Overall Joint Accuracy - %f \\n\" % joint_accuracy(cand1,ref1,d_c = False,type2_c = False))\n",
    "    #file1.write(\"Overall Slot Accuracy - %f \\n\" % slot_accuracy(cand1,ref1))\n",
    "    #file1.close()\n",
    "    return total_loss / total_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "I7O5qnlBy3K-"
   },
   "outputs": [],
   "source": [
    "file9 = open(\"loss_file8.txt\", \"w\") \n",
    "L = [\"This is a file containing losses and accuracies of train and validation data \\n\"] \n",
    "file9.writelines(L)\n",
    "file9.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleLossCompute:\n",
    "    \"A simple loss compute and train function.\"\n",
    "    def __init__(self, generator,generator1, criterion, opt=None):\n",
    "        self.generator = generator\n",
    "        self.generator1 = generator1\n",
    "        self.criterion = criterion\n",
    "        self.opt = opt\n",
    "        \n",
    "    def __call__(self, x1,x2, y,z, norm1,norm2):\n",
    "        x1 = self.generator(x1)\n",
    "        x2 = self.generator1(x2)\n",
    "        loss1 = self.criterion(x1.contiguous().view(-1, x1.size(-1)), \n",
    "                              y.contiguous().view(-1)) / norm1\n",
    "        loss2 = self.criterion(x2.contiguous().view(-1, x2.size(-1)), \n",
    "                              z.contiguous().view(-1)) / norm2\n",
    "        loss = loss1 + loss2\n",
    "        loss.backward()\n",
    "        if self.opt is not None:\n",
    "            self.opt.step()\n",
    "            self.opt.optimizer.zero_grad()\n",
    "        return loss.item() * norm1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "88ZW0ngFOZ3n"
   },
   "outputs": [],
   "source": [
    "model = torch.load('simpletod/dialog_NLP.pt')     #run this cell if there is a previously saved model like the previous cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\alipu\\Desktop\\TransformerDialog_ArnabM\\JointBeliefAct\\model_joint.py:315: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n",
      "  nn.init.xavier_uniform(p)\n"
     ]
    }
   ],
   "source": [
    "V = 30000\n",
    "model = make_model(V, V, N=2) #run this cell if there is no saved model like the previous cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = LabelSmoothing(size=V, padding_idx=0, smoothing=0.0)\n",
    "\n",
    "model_opt = NoamOpt(model.src_embed[0].d_model, 1, 400,\n",
    "        torch.optim.Adam(model.parameters(), lr=0.001, betas=(0.9, 0.98), eps=1e-9))\n",
    "#q = 7\n",
    "model.to(device)\n",
    "for epoch in range(50):\n",
    "    model.train()\n",
    "    run_epoch(data_gen(V, 10, 5670,s1,t1,a1), model, \n",
    "              SimpleLossCompute(model.generator,model.generator1, criterion, model_opt))\n",
    "    model.eval()\n",
    "    l1 = run_epoch(data_gen(V, 10,737 ,s1,t1,a1), model, \n",
    "              SimpleLossCompute(model.generator,model.generator1, criterion, model_opt))\n",
    "    l = run_epoch(data_gen(V, 10, 737,s9,t9,a9), model, \n",
    "                    SimpleLossCompute(model.generator,model.generator1, criterion, None))\n",
    "    torch.save(model,'simpletod/dialog_NLP.pt')\n",
    "    file1 = open(\"loss_now.txt\", \"a\")\n",
    "    file1.write(\"Epoch %d \\n\" % (epoch+1)) # append mode \n",
    "    file1.write(\"Train loss %f \\n\" % l1) \n",
    "    file1.write(\"Validation loss %f \\n\" % l)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "NLPtask.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
